# æ¨èç³»ç»Ÿç§‘ç ”é¡¹ç›® - RaSeRecç®€åŒ–ç‰ˆæœ¬ Cursorå¼€å‘è§„èŒƒ
# ==============================================

## é¡¹ç›®æ¦‚è¿°
æœ¬é¡¹ç›®åŸºäºé¢„è®­ç»ƒçš„DuoRecæ¨¡å‹ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºæŠ€æœ¯æå‡æ¨èæ€§èƒ½ã€‚æ ¸å¿ƒè®­ç»ƒç­–ç•¥æ˜¯é€šè¿‡KLæ•£åº¦ä¼˜åŒ–æ£€ç´¢å™¨ç¼–ç å™¨ï¼Œä½¿æ£€ç´¢å™¨è¯„åˆ†åˆ†å¸ƒæ¥è¿‘é¢„è®­ç»ƒæ¨èå™¨çš„Transformeræ³¨æ„åŠ›æƒé‡åˆ†å¸ƒã€‚èåˆæœºåˆ¶é‡‡ç”¨ç®€å•çš„ç‚¹ç§¯ç›¸ä¼¼åº¦ï¼Œé‡ç‚¹åœ¨äºè®­ç»ƒæ£€ç´¢å™¨çš„æŸ¥è¯¢å˜æ¢èƒ½åŠ›ã€‚

## æ ¸å¿ƒè®­ç»ƒé€»è¾‘

### 1. **é¢„è®­ç»ƒæ¨¡å‹åŠ è½½**
- åŠ è½½é¢„è®­ç»ƒçš„DuoRecæ¨¡å‹ï¼ˆåŸºäºTransformerè‡ªæ³¨æ„åŠ›çš„åºåˆ—æ¨èå™¨ï¼‰
- å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œä»…è®­ç»ƒæ£€ç´¢å™¨ç¼–ç å™¨
- æ„å»ºçŸ¥è¯†åº“ï¼šç¼“å­˜è®­ç»ƒæ•°æ®çš„åºåˆ—è¡¨ç¤ºå’Œç›®æ ‡é¡¹åµŒå…¥

### 2. **ä¸¤ä¸ªè¯„åˆ†åˆ†å¸ƒçš„å®šä¹‰**

#### ğŸ¯ **æ£€ç´¢å™¨è¯„åˆ†åˆ†å¸ƒ P_R(d|x)**
```python
def compute_retrieval_scores(self, seq_output, retrieved_seqs, retrieved_tars, pos_items):
```

#### ğŸ¯ **æ¨èå™¨è¯„åˆ†åˆ†å¸ƒ Q_M(d|x,y)**
```python
def compute_recommendation_scores(self, seq_output, retrieved_seqs, retrieved_tars):
```

### 3. **KLæ•£åº¦è®­ç»ƒç›®æ ‡**
```python
def calculate_loss(self, interaction):
    """
    L = (1/|B|) * sum_x KL(P_R(d|x) || Q_M(d|x,y))
    ä¼˜åŒ–æ£€ç´¢å™¨åˆ†å¸ƒä½¿å…¶æ¥è¿‘æ¨èå™¨çš„æ³¨æ„åŠ›åˆ†å¸ƒ
    """
    retrieval_probs = self.compute_retrieval_scores(...)    # æ£€ç´¢å™¨åˆ†å¸ƒ
    recommendation_probs = self.compute_recommendation_scores(...)  # é¢„è®­ç»ƒæ³¨æ„åŠ›åˆ†å¸ƒ
    
    kl_loss = torch.sum(retrieval_probs * torch.log(retrieval_probs / recommendation_probs), dim=-1)
    return kl_loss.mean()
```

## æŠ€æœ¯æ ˆå’Œæ¡†æ¶
- **ä¸»æ¡†æ¶**: RecBole (æ¨èç³»ç»Ÿå¼€æºæ¡†æ¶)
- **é¢„è®­ç»ƒæ¨¡å‹**: DuoRec (åŸºäºTransformerçš„åºåˆ—æ¨èå™¨)
- **æ·±åº¦å­¦ä¹ **: PyTorch 2.4.1+
- **æ£€ç´¢æŠ€æœ¯**: FAISS, è‡ªå®šä¹‰æ£€ç´¢å™¨ç¼–ç å™¨(MLP)
- **èåˆæœºåˆ¶**: ç‚¹ç§¯ç›¸ä¼¼åº¦ + åŠ æƒå¹³å‡ï¼ˆæ›¿ä»£å¤æ‚æ³¨æ„åŠ›ï¼‰
- **æ³¨æ„åŠ›æå–**: Transformer MultiHeadAttentionæƒé‡
- **æ•°æ®å¤„ç†**: pandas, numpy, scikit-learn

## æ¨¡å‹æ¶æ„é‡ç‚¹

### 1. **å®Œæ•´è®­ç»ƒæµç¨‹**
```
1. åŠ è½½é¢„è®­ç»ƒDuoRecæ¨¡å‹ â†’ å†»ç»“æ‰€æœ‰å‚æ•°
2. æ„å»ºçŸ¥è¯†åº“ â†’ ç¼“å­˜åºåˆ—è¡¨ç¤ºå’Œç›®æ ‡é¡¹
3. è®­ç»ƒæ£€ç´¢å™¨ç¼–ç å™¨ï¼š
   â”œâ”€â”€ æ£€ç´¢å™¨æŸ¥è¯¢å˜æ¢: MLP(seq_output) 
   â”œâ”€â”€ FAISSæ£€ç´¢: è·å–top-kç›¸ä¼¼åºåˆ—
   â”œâ”€â”€ è®¡ç®—æ£€ç´¢å™¨åˆ†å¸ƒ: èåˆâ†’é¢„æµ‹â†’softmax
   â”œâ”€â”€ è®¡ç®—æ¨èå™¨åˆ†å¸ƒ: Transformeræ³¨æ„åŠ›æƒé‡
   â””â”€â”€ KLæ•£åº¦æŸå¤±: ä¼˜åŒ–æ£€ç´¢å™¨æ¥è¿‘æ¨èå™¨
```

### 2. **å…³é”®è®¾è®¡åŸåˆ™**
- **åªè®­ç»ƒæ£€ç´¢å™¨**: é¢„è®­ç»ƒæ¨¡å‹å‚æ•°å®Œå…¨å†»ç»“
- **æ³¨æ„åŠ›æƒé‡æå–**: ä½¿ç”¨é¢„è®­ç»ƒTransformerçš„Query/Keyå˜æ¢
- **ç®€åŒ–èåˆ**: ç‚¹ç§¯ç›¸ä¼¼åº¦æ›¿ä»£å¤æ‚äº¤å‰æ³¨æ„åŠ›
- **åˆ†å¸ƒå¯¹é½**: KLæ•£åº¦ç¡®ä¿æ£€ç´¢å™¨å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹çš„åå¥½

### 3. **å‚æ•°å†»ç»“ç­–ç•¥**
```python
def _freeze_parameters(self):
    # å†»ç»“æ‰€æœ‰å‚æ•°
    for param in self.parameters():
        param.requires_grad = False
    
    # åªè§£å†»æ£€ç´¢å™¨ç¼–ç å™¨
    for param in self.retriever_mlp.parameters():
        param.requires_grad = True
    for param in self.retriever_layer_norms.parameters():
        param.requires_grad = True
```

## æ ¸å¿ƒç®—æ³•å®ç°è§„èŒƒ

### 1. **æ£€ç´¢å™¨ç¼–ç å™¨å‰å‘ä¼ æ’­**
```python
def retriever_forward(self, seq_output):
    """MLPå¯¹åºåˆ—è¡¨ç¤ºè¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå­¦ä¹ æ›´å¥½çš„æ£€ç´¢æŸ¥è¯¢"""
    hidden = seq_output
    for layer, layer_norm in zip(self.retriever_mlp, self.retriever_layer_norms):
        residual = hidden
        hidden = layer(hidden)
        hidden = self.retriever_act_fn(hidden)
        hidden = self.retriever_dropout_layer(hidden)
        hidden = layer_norm(hidden + residual)  # æ®‹å·®è¿æ¥
    return hidden
```

### 2. **æ³¨æ„åŠ›æƒé‡æå–çš„æ ¸å¿ƒé€»è¾‘**
```python
# å…³é”®ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„Query/Keyå˜æ¢çŸ©é˜µ
query = first_attention_layer.query(seq_output.unsqueeze(1))
key = first_attention_layer.key(retrieved_seq.unsqueeze(1))

# è®¡ç®—å¤šå¤´æ³¨æ„åŠ›åˆ†æ•°
attention_scores = torch.matmul(query, key.transpose(-1, -2))
attention_scores = attention_scores / sqrt(attention_head_size)

# å¹³å‡å¤šå¤´å¾—åˆ°æœ€ç»ˆæƒé‡
attention_weight = attention_scores.mean(dim=2)
```

### 3. **æ•°å€¼ç¨³å®šæ€§ä¿è¯**
```python
# KLæ•£åº¦è®¡ç®—
epsilon = 1e-8
retrieval_probs = retrieval_probs + epsilon
recommendation_probs = recommendation_probs + epsilon
kl_div = torch.sum(retrieval_probs * torch.log(retrieval_probs / recommendation_probs), dim=-1)

# æ¸©åº¦ç¼©æ”¾
logits = scores / self.temperature
probs = torch.softmax(logits, dim=1)
```

## é…ç½®æ–‡ä»¶è§„èŒƒ

### 1. **æ ¸å¿ƒå‚æ•°è®¾ç½®**
```yaml
# é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
pretrained_path: "./log/RaSeRec/Amazon_Beauty/bs1024-.../model.pth"

# æ¨¡å‹åŸºç¡€å‚æ•°
model: RaSeRec
hidden_size: 64          # ä¸é¢„è®­ç»ƒæ¨¡å‹ä¿æŒä¸€è‡´
n_layers: 2              # Transformerå±‚æ•°
n_heads: 2               # æ³¨æ„åŠ›å¤´æ•°

# æ£€ç´¢å‚æ•°
top_k: 10                # æ£€ç´¢åºåˆ—æ•°é‡
alpha: 0.5               # èåˆæƒé‡(åŸåºåˆ—:æ£€ç´¢åºåˆ— = 0.5:0.5)
nprobe: 1                # FAISSæ¢æµ‹å‚æ•°

# æ£€ç´¢å™¨ç¼–ç å™¨å‚æ•°
retriever_layers: 1         # MLPå±‚æ•°
retriever_temperature: 0.1  # æ£€ç´¢åˆ†å¸ƒæ¸©åº¦
recommendation_temperature: 0.1  # æ¨èåˆ†å¸ƒæ¸©åº¦
retriever_dropout: 0.1      # Dropoutç‡
kl_weight: 1               # KLæŸå¤±æƒé‡
```

### 2. **è®­ç»ƒå‚æ•°ä¼˜åŒ–**
```yaml
# è®­ç»ƒè®¾ç½®
epochs: 10
learning_rate: 0.001     # æ£€ç´¢å™¨å­¦ä¹ ç‡
train_batch_size: 1024
eval_step: 1
stopping_step: 5

# ç”±äºåªè®­ç»ƒæ£€ç´¢å™¨ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
# weight_decay: 0  # æ£€ç´¢å™¨å‚æ•°é‡å°‘ï¼Œä¸éœ€è¦å¼ºæ­£åˆ™åŒ–
```

## å®éªŒè®¾ç½®è§„èŒƒ

### 1. **å…³é”®æ¶ˆèå®éªŒ**
- **æ£€ç´¢å™¨å±‚æ•°**: 1å±‚ vs 2å±‚ vs 3å±‚ MLP
- **æ¸©åº¦å‚æ•°**: retriever_temperatureå’Œrecommendation_temperatureçš„æ•æ„Ÿæ€§
- **èåˆæƒé‡alpha**: 0.1, 0.3, 0.5, 0.7, 0.9
- **æ£€ç´¢æ•°é‡top_k**: 5, 10, 20, 50
- **æ³¨æ„åŠ›å±‚é€‰æ‹©**: ç¬¬1å±‚ vs æœ€å1å±‚ vs å¤šå±‚å¹³å‡

### 2. **é‡è¦åŸºçº¿å¯¹æ¯”**
- **æ— æ£€ç´¢å¢å¼º**: çº¯é¢„è®­ç»ƒDuoRec
- **éšæœºæ£€ç´¢**: éšæœºé€‰æ‹©æ£€ç´¢åºåˆ—
- **ç®€å•ç›¸ä¼¼åº¦**: ä¸ä½¿ç”¨æ³¨æ„åŠ›æƒé‡çš„ç‰ˆæœ¬
- **å®Œæ•´RaSeRec**: ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›çš„åŸå§‹ç‰ˆæœ¬

### 3. **è¯„ä¼°é‡ç‚¹**
- **æ¨èæ€§èƒ½**: Recall@K, NDCG@K
- **åˆ†å¸ƒå¯¹é½**: KLæ•£åº¦æ”¶æ•›æƒ…å†µ
- **æ£€ç´¢è´¨é‡**: æ£€ç´¢å™¨æ‰¾åˆ°çš„åºåˆ—ä¸é¢„è®­ç»ƒæ¨¡å‹åå¥½çš„ä¸€è‡´æ€§
- **è®­ç»ƒæ•ˆç‡**: åªè®­ç»ƒæ£€ç´¢å™¨çš„é€Ÿåº¦ä¼˜åŠ¿

## è°ƒè¯•å’ŒéªŒè¯æŒ‡å—

### 1. **åˆ†å¸ƒå¯¹é½æ£€æŸ¥**
```python
# ç›‘æ§KLæ•£åº¦æ”¶æ•›
print(f"KL divergence: {kl_loss.item():.4f}")

# æ£€æŸ¥ä¸¤ä¸ªåˆ†å¸ƒçš„ç›¸ä¼¼æ€§
cosine_sim = F.cosine_similarity(retrieval_probs, recommendation_probs, dim=1)
print(f"Distribution similarity: {cosine_sim.mean():.4f}")

# éªŒè¯æ³¨æ„åŠ›æƒé‡æå–
print(f"Attention weights range: [{recommendation_probs.min():.4f}, {recommendation_probs.max():.4f}]")
print(f"Attention weights sum: {recommendation_probs.sum(dim=1)}")  # åº”è¯¥æ¥è¿‘1
```

### 2. **æ¢¯åº¦æµæ£€æŸ¥**
```python
# ç¡®ä¿åªæœ‰æ£€ç´¢å™¨æœ‰æ¢¯åº¦
for name, param in self.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm = {param.grad.norm():.6f}")
    else:
        print(f"{name}: NO GRADIENT (expected for frozen params)")
```

### 3. **é¢„è®­ç»ƒæ¨¡å‹åŠ è½½éªŒè¯**
```python
# éªŒè¯é¢„è®­ç»ƒå‚æ•°åŠ è½½
print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°!")
print(f"ç¼ºå¤±çš„å‚æ•°é”®: {missing_keys}")
print(f"å¤šä½™çš„å‚æ•°é”®: {unexpected_keys}")

# æ£€æŸ¥æ¨¡å‹æ¶æ„åŒ¹é…
assert hasattr(self.trm_encoder.layer[0], 'multi_head_attention')
print("Transformeræ³¨æ„åŠ›å±‚åŠ è½½æˆåŠŸ!")
```

## æ ¸å¿ƒè´¡çŒ®å’Œåˆ›æ–°ç‚¹

### 1. **ç†è®ºè´¡çŒ®**
- **åˆ†å¸ƒå¯¹é½å­¦ä¹ **: é€šè¿‡KLæ•£åº¦è®©æ£€ç´¢å™¨å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›åå¥½
- **æŸ¥è¯¢å˜æ¢ä¼˜åŒ–**: ä¸“é—¨è®­ç»ƒæ£€ç´¢å™¨çš„æŸ¥è¯¢ç¼–ç èƒ½åŠ›
- **ç®€åŒ–é«˜æ•ˆèåˆ**: ç‚¹ç§¯æ›¿ä»£å¤æ‚æ³¨æ„åŠ›ï¼Œä¿æŒæ€§èƒ½é™ä½å¤æ‚åº¦

### 2. **æŠ€æœ¯åˆ›æ–°**
- **æ³¨æ„åŠ›æƒé‡æå–**: ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒTransformerçš„Query/Keyå˜æ¢
- **å‚æ•°é«˜æ•ˆè®­ç»ƒ**: åªè®­ç»ƒæ£€ç´¢å™¨ï¼Œå¤§å¹…å‡å°‘è®­ç»ƒæˆæœ¬
- **çŸ¥è¯†è’¸é¦æ€æƒ³**: å°†é¢„è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›çŸ¥è¯†è’¸é¦åˆ°æ£€ç´¢å™¨

### 3. **å®éªŒéªŒè¯è¦ç‚¹**
- è¯æ˜æ£€ç´¢å™¨åˆ†å¸ƒç¡®å®å­¦ä¼šäº†é¢„è®­ç»ƒæ¨¡å‹çš„åå¥½
- éªŒè¯ç®€åŒ–èåˆæœºåˆ¶ä¸ä¼šæ˜¾è‘—æŸå¤±æ€§èƒ½
- å¯¹æ¯”è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ•ˆæœçš„æƒè¡¡

## æ³¨æ„äº‹é¡¹

1. **é¢„è®­ç»ƒæ¨¡å‹ä¾èµ–**: å¿…é¡»æœ‰é«˜è´¨é‡çš„é¢„è®­ç»ƒDuoRecæ¨¡å‹
2. **æ¶æ„åŒ¹é…**: ç¡®ä¿RaSeRecä¸é¢„è®­ç»ƒæ¨¡å‹çš„hidden_sizeç­‰å‚æ•°ä¸€è‡´
3. **æ¸©åº¦å‚æ•°é‡è¦æ€§**: ä¸¤ä¸ªåˆ†å¸ƒçš„æ¸©åº¦å‚æ•°éœ€è¦ä»”ç»†è°ƒä¼˜
4. **æ³¨æ„åŠ›å±‚é€‰æ‹©**: ä¸åŒTransformerå±‚çš„æ³¨æ„åŠ›æƒé‡å¯èƒ½æœ‰ä¸åŒæ•ˆæœ
5. **æ£€ç´¢åº“è´¨é‡**: çŸ¥è¯†åº“çš„æ„å»ºè´¨é‡ç›´æ¥å½±å“æœ€ç»ˆæ•ˆæœ

éµå¾ªä»¥ä¸Šè§„èŒƒå¯ä»¥ç¡®ä¿æ­£ç¡®å®ç°åŸºäºé¢„è®­ç»ƒDuoRecçš„æ£€ç´¢å¢å¼ºæ¨èç³»ç»Ÿã€‚ 